{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#introduction","title":"Introduction","text":"<p>In this project, I created a simple ETL data pipeline to work with different types of datasets. The pipeline takes the data, changes it into a clean format, and saves it as CSV files. All steps of the process are written into a log file, so it is easy to follow what happened and fix problems if needed.</p>"},{"location":"#data-flow","title":"Data Flow","text":""},{"location":"#folder-structure","title":"Folder Structure","text":"<pre><code>SimpleDataFlow/\n\u2502\n\u251c\u2500\u2500 data/                  \n\u2502   \u251c\u2500\u2500 raw/               \n\u2502   \u2514\u2500\u2500 processed/         \n\u2502\n\u251c\u2500\u2500 logs/                  \n\u2502   \u2514\u2500\u2500 log_file.txt\n\u2502\n\u251c\u2500\u2500 docs/ \n\u2502   \u2514\u2500\u2500 index.md\n\u2502\n\u251c\u2500\u2500 src/                     \n\u2502   \u2514\u2500\u2500 etl.py\n\u2502\n\u251c\u2500\u2500 requirements.txt       \n\u251c\u2500\u2500 README.md              \n\u251c\u2500\u2500 mkdocs.yaml\n\u2514\u2500\u2500 LICANSE      \n</code></pre>"}]}